#' fit.model.set
#'
#' Conducts a full subsets analysis based on gam(m4) using the list generated by a call to generate.model.set
#' @param model.set.list A list as returned by generate.model.set()
#'
#' @param max.models The total number of models allowed to be fit and the model fits to be saved during fitting. If the candidate set is bigger than this value, a warning message will be returned.
#'
#' @param save.model.fits Should all successfully fitted models be saved to list success.models. Defaults to TRUE. Value is overwritten if the number of models n the set is bigger than max.models.
#'
#' @param parallel  A logical value indicating if parallel processing should be used. The default is FALSE.
#'
#' @param r2.type The value to extract from the gam model fit to use as the R squared value. Defaults to r2.lm.est which returns and estimated R squared value based on a linear regression between the observed and predicted values. r2 will return the adjusted R.sq as reported by gam, gamm or gamm4.dev will return the deviance explained as reported by gam or gamm. Note gamm4 does not currently return a deviance.
#'
#' @param  report.unique.r2 The estimated null model R2 is subtracted from each model R2 to give an idea of the unique variance explained. This can be useful where null terms are included in the model set.
#'
#' @details The function constructs and fits a complete model set based on the supplied arguments.
#' for more information see Fisher R, Wilson SK, Sin TM, Lee AC, Langlois TJ (2018) A simple function for full-subsets multiple regression in ecology with R. Ecology and Evolution
#' https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.4134
#' @export
#' @return A list of the following output files:
#'
#' mod.data.out - A data.frame that contains the statistics associated with each model fit. This includes AICc and BIC, delta values (e.g. AICc-(min(AICc)), corresponding weight values (Burnham and Anderson 2003), an estimate of the model R2, and a column for each of the included predictor variables containing either 0 (variable not included in the model) or 1 (variable is present in the model).
#' Use of BIC in information theoretic approaches has been heavily criticised because of the inherent assumption of BIC that there is a true model that is represented in the candidate set (Anderson & Burnham 2002). Rather than decide a-priori which model selection tool users should adopt, we supply both as part of the function outputs.
#' To simplify output, only AICc and AICc based model weights, rather than AIC, are included as these are asymptotically equivalent at large sample sizes, and for small sample sizes AICc should be used in any case.
#' Calculating R2 values is non-trivial for mixed models, especially non-gaussian cases (and some argue should not be done at all). We have supplied a range of methods for estimating R2 (r2.type), as in our experience a single method rarely performs adequately across all scenarios.
#'
#' used.data - A data.frame which is identical to the data.frame initially supplied by the user, but with any hard coded interaction terms appended via cbind.
#'
#' failed.models - A list of model formula that failed to fit. Ideally the list of failed models should be empty, but when this is not the case interrogating failed.models provides a useful means of troubleshooting. Users can examine which models are not fitting and explore the reasons for this by fitting the failed models outside the full.subsets.gam call based on the listed formula. When a large number of models fail to fit properly it usually indicates poor specification of the initial test.fit or other arguments in the call to full.subsets.gam (such as the inclusion of factor interactions when there are few data within each level of the factor), or that inappropriate variables are being included in the model set.
#'
#' success.models - A complete list of all successfully fitted model formula. If models were saved, this can be used for multimodel inference and creating model averaged predictions.
#' otherwise the formula can be used to refit the top model set via a call to update of the test fit: update(test.fit,formula=mod.formula[[l]])
#'
#' variable.importance - A list containing importance scores for each included predictor.
#' To determine the relative importance of each predictor across the whole model set we summed the ?i values for all models containing each variable. The higher the combined weights for an explanatory parameter, the more important it is in the analysis (Burnham & Anderson, 2002). An assumption of the use of summed model weights to infer variable importance is that the number of models in which the different predictors are present is uniform. As our function removes models with correlated predictors, this is not always the case. To overcome this issue, the summed variable.importance scores are the summed weights for the best n models, where n is equal to the minimum number of models any one predictor is present in.

fit.model.set=function(model.set.list,
                          max.models=100,
                          save.model.fits=T,
                          parallel=F,
                          n.cores=4,
                          r2.type="r2.lm.est",
                          report.unique.r2=F){
  # make sure use.dat is a data.frame
  use.datModSet <- model.set.list$used.data
  n.mods=model.set.list$n.mods
  mod.formula=model.set.list$mod.formula
  test.fit=model.set.list$test.fit
  included.vars=model.set.list$included.vars

  if(n.mods>max.models){
        warning(paste("You have ",n.mods," models. Individual models fits will not be saved.
        If you want to save all the model fits all of these you need to
        increase 'max.models' from ",max.models,".",sep=""))
        save.model.fits=F
       }

  # some functions for extracting model information
  require(MuMIn)

  ## Now do the model fitting
  # if all model fits are to be saved
  if(save.model.fits==T){
    # now fit the models by updating the test fit (with or without parallel)
    pb <- txtProgressBar(max = length(mod.formula), style = 3)
    progress <- function(n) setTxtProgressBar(pb, n)
    if(parallel==T){
     require(doSNOW)
     cl=makeCluster(n.cores)
     registerDoSNOW(cl)
     opts <- list(progress = progress)
     out.dat<-foreach(l = 1:length(mod.formula),
                     .packages=c('mgcv','gamm4','MuMIn','FSSgam'),
                     .errorhandling='pass',
                     .options.snow = opts)%dopar%{
       fit.mod.l(mod.formula[[l]],test.fit.=test.fit,use.dat.=use.datModSet)
    }
     close(pb)
     stopCluster(cl)
     registerDoSEQ()
             }else{
        out.dat=list()
        for(l in 1:length(mod.formula)){
           mod.l=fit.mod.l(mod.formula[[l]],test.fit.=test.fit,use.dat.=use.datModSet)
           out.dat=c(out.dat,list(mod.l))
          setTxtProgressBar(pb,l)
           }
    }
    close(pb)
    names(out.dat)=names(mod.formula[1:n.mods])

    # find all the models that didn't fit and extract the error messages
    model.success=lapply(lapply(out.dat,FUN=class),FUN=function(x){
       length(grep("gam",x))>0})
    failed.models=mod.formula[which(model.success==F)]
    success.models=out.dat[which(model.success==T)]
    if(length(success.models)==0){
          stop("None of your models fitted successfully. Please check your input objects.")}

    # of the successful models, make a table indicating which variables are included
    var.inclusions=build.inclusion.mat(included.vars=included.vars,formula.list=success.models)
    # now make a table of all the model summary data
    mod.data.out=data.frame("modname"=names(success.models))
    mod.data.out$formula=unlist(lapply(success.models,FUN=function(x){as.character(formula(x)[3])}))
    mod.data.out=cbind(mod.data.out,do.call("rbind",lapply(success.models,FUN=function(x){
                        unlist(extract.mod.dat(x,r2.type.=r2.type))})))

  }else{ # if model fits are not to be saved
    #for all models make a table indicating which variables are included
    var.inclusions=build.inclusion.mat(included.vars=included.vars,formula.list=mod.formula)
     # now make a table of all the model summary data
    mod.data.out=data.frame("modname"=names(mod.formula))
    mod.data.out$formula=unlist(lapply(mod.formula,FUN=function(x){as.character(formula(x))[2]}))

        # now fit the models by updating the test fit (with or without parallel)
    pb <- txtProgressBar(max = length(mod.formula), style = 3)
    progress <- function(n) setTxtProgressBar(pb, n)
    if(parallel==T){
     require(doSNOW)
     cl=makeCluster(n.cores)
     registerDoSNOW(cl)
     opts <- list(progress = progress)
     mod.dat<<-foreach(l = 1:length(mod.formula),
                     .packages=c('mgcv','gamm4','MuMIn','FSSgam'),
                     #.errorhandling='pass',
                     .options.snow = opts)%dopar%{
        unlist(extract.mod.dat(fit.mod.l(mod.formula[[l]],test.fit.=test.fit,use.dat.=use.datModSet),
                               r2.type.=r2.type))
     }
     close(pb)
     stopCluster(cl)
     registerDoSEQ()
             }else{
        mod.dat=list()
        for(l in 1:length(mod.formula)){
          mod.l=fit.mod.l(mod.formula[[l]],test.fit.=test.fit,use.dat.=use.datModSet)
          out=unlist(extract.mod.dat(mod.l,r2.type.=r2.type))
          mod.dat=c(mod.dat,list(out))
          setTxtProgressBar(pb,l)
          }
    }
    close(pb)
    names(mod.dat)=names(mod.formula[1:n.mods])
    mod.data.out=cbind(mod.data.out,do.call("rbind",mod.dat))

    failed.models=mod.formula[which(is.na(mod.data.out$AICc)==T)]
    success.models=mod.formula[which(is.na(mod.data.out$AICc)==F)]
  }

  mod.data.out$delta.AICc=round(mod.data.out$AICc-min(mod.data.out$AICc,na.rm=T),3)
  mod.data.out$delta.BIC=round(mod.data.out$BIC-min(mod.data.out$BIC,na.rm=T),3)
  mod.data.out$wi.AICc=round(wi(mod.data.out$AICc),3)
  mod.data.out$wi.BIC=round(wi(mod.data.out$BIC),3)

  # substract the null model r2 value from each model r2 value
  if(report.unique.r2==T){
   null.r2=mod.data.out$r2.vals[which(mod.data.out$modname=="null")]
   mod.dat$r2.vals.unique=mod.data.out$r2.vals-null.r2
   }

  ### now add columns for the included predictors to the dataframe
  mod.data.out=cbind(mod.data.out,var.inclusions)

  # now calculate the variable importance
   # find the min number of models for each variable
  min.mods=min(colSums(mod.data.out[,included.vars]))
  # first for AICc
  var.weights=unlist(lapply(included.vars,FUN=function(x){
           sum(sort(mod.data.out$wi.AICc[which(mod.data.out[,x]==1)],decreasing=T)[1:min.mods])}))
  names(var.weights)=included.vars
  variable.weights.raw=var.weights
  #variable.weights.raw=colSums(mod.data.out[,included.vars]*mod.data.out$wi.AICc)
  aic.var.weights=list(variable.weights.raw=variable.weights.raw)
  # next for BIC
  var.weights=unlist(lapply(included.vars,FUN=function(x){
           sum(sort(mod.data.out$wi.BIC[which(mod.data.out[,x]==1)],decreasing=T)[1:min.mods])}))
  names(var.weights)=included.vars
  variable.weights.raw=var.weights
  #variable.weights.raw=colSums(mod.data.out[,included.vars]*mod.data.out$wi.BIC)
  bic.var.weights=list(variable.weights.raw=variable.weights.raw)
  # now return the list of outputs
  return(list(mod.data.out=mod.data.out,
              failed.models=failed.models,
              success.models=success.models,
              variable.importance=
                 list(aic=aic.var.weights,bic=bic.var.weights)))
} #------------------ end function --------------------------------------------#